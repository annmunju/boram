{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broad-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1bcb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, AlbertModel, BertModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import hanja\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1de3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95287ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH + 'train_감정분류.csv', index_col=0)\n",
    "test = pd.read_csv(PATH + 'test_감정분류.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9abd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = list(train['감정_소분류'].unique())\n",
    "\n",
    "emotion_ls = []\n",
    "for t in train['감정_소분류']:\n",
    "    emotion_ls.append(emotion.index(t))\n",
    "    \n",
    "train['감정_대분류'] = emotion_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83818b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = list(test['감정_소분류'].unique())\n",
    "\n",
    "emotion_ls = []\n",
    "for t in test['감정_소분류']:\n",
    "    emotion_ls.append(emotion.index(t))\n",
    "    \n",
    "test['감정_대분류'] = emotion_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ada95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['응답정리']\n",
    "y_train = train['감정_대분류']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25730324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "228463    1\n",
       "228464    1\n",
       "228465    1\n",
       "228466    1\n",
       "228467    1\n",
       "Name: 감정_대분류, Length: 228468, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2157c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test['응답정리']\n",
    "y_test = test['감정_대분류']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b20516",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "339e8012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert_kor_base = BertTokenizerFast.from_pretrained(\"kykim/albert-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09db2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_t = x_train.to_numpy().reshape(-1,1)\n",
    "labels_t = y_train.to_numpy().reshape(-1,1)\n",
    "# oversample = RandomOverSampler()\n",
    "# X_over, y_over = oversample.fit_resample(titles_t, labels_t)\n",
    "# train = pd.DataFrame({'title':X_over.reshape(-1), 'topic_idx':y_over.reshape(-1)})\n",
    "train = pd.DataFrame({'title':titles_t.reshape(-1), 'topic_idx':labels_t.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dd7d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_t = x_test.to_numpy().reshape(-1,1)\n",
    "labels_t = y_test.to_numpy().reshape(-1,1)\n",
    "# oversample = RandomOverSampler()\n",
    "# X_over, y_over = oversample.fit_resample(titles_t, labels_t)\n",
    "# train = pd.DataFrame({'title':X_over.reshape(-1), 'topic_idx':y_over.reshape(-1)})\n",
    "test = pd.DataFrame({'title':titles_t.reshape(-1), 'topic_idx':labels_t.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e825330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 클래스 생성\n",
    "class NewsSubjectDataset(Dataset):\n",
    "  def __init__(self, subjects, targets, tokenizer, max_len):\n",
    "    self.subjects = subjects\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.subjects)\n",
    "  def __getitem__(self, item):\n",
    "    subject = str(self.subjects[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      subject,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return {\n",
    "      'subject_text': subject,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size, shuffle_=False, valid=False):\n",
    "  if valid:\n",
    "    ds = NewsSubjectDataset(\n",
    "      subjects=df.title.to_numpy(),\n",
    "      targets=np.zeros(len(df)),\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "      )\n",
    "  else:\n",
    "    ds = NewsSubjectDataset(\n",
    "      subjects=df.title.to_numpy(),\n",
    "      targets=df.topic_idx.to_numpy(),\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "    )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle = shuffle_\n",
    "  )\n",
    "  \n",
    "# 데이터 로더 생성\n",
    "BATCH_SIZE =64\n",
    "MAX_LEN =32\n",
    "train_data_loader = create_data_loader(train, tokenizer_bert_kor_base, MAX_LEN, BATCH_SIZE, shuffle_=True)\n",
    "valid_data_loader = create_data_loader(test, tokenizer_bert_kor_base, MAX_LEN, BATCH_SIZE, valid=True)\n",
    "import random\n",
    "class NewsSubjectClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(NewsSubjectClassifier, self).__init__()\n",
    "    self.bert = AlbertModel.from_pretrained(\"kykim/albert-kor-base\")\n",
    "    self.drop = nn.Dropout(p=0.5)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "       return_dict=False\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  subject_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      texts = d[\"subject_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      subject_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(outputs)\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  return subject_texts, predictions, prediction_probs\n",
    "import gc\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a7739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/albert-kor-base were not used when initializing AlbertModel: ['sop_classifier.classifier.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'sop_classifier.classifier.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_albert_kor_base = NewsSubjectClassifier(n_classes=58).to(device)\n",
    "optimizer = AdamW(model_albert_kor_base.parameters(), lr=1e-4)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "## cosine_scheduler or linear,  warmup or no warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=int(total_steps*0.1),\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for d in tqdm(data_loader):\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 397/3570 [33:49<5:47:45,  6.58s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model_albert_kor_base,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "y_review_texts, y_pred, y_pred_probs = get_predictions(\n",
    "  model_albert_kor_base,\n",
    "  valid_data_loader\n",
    ")\n",
    "answers = []\n",
    "answers.append(y_pred.tolist())\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-realtor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
